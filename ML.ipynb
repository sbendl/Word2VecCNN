{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Portions of this notebook adapted from https://github.com/rouseguy/DeepLearning-NLP/blob/master/notebooks/4.%20Recurrent%20Neural%20Networks.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stephen\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(r'F:\\NeuralNetwork\\word2vec\\GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras.engine import Layer\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "\n",
    "from keras.layers import *\n",
    "from keras import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stephen\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import words as english_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from string import *\n",
    "from time import *\n",
    "from multiprocessing.dummy import Pool\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ew_set = set(english_words.words())\n",
    "# reg_vocab = [word for word in model.vocab if word in ew_set]\n",
    "#reg_vocab = [word for word in model.vocab if all(letter in printable for letter in word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reg_vocab = [word for word in model.vocab if word in ew_set]\n",
    "reg_vocab = ['<' + word + '>' for word in model.vocab if all(letter not in {'#', '@', '.', ',', '/', '\\\\', '<', '>', *ascii_uppercase} and letter in printable for letter in word)]\n",
    "reg_vocab_set = set(reg_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_list = printable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(char_list))\n",
    "indices_char = dict((i, c) for i, c in enumerate(char_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 383866\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 0\n",
    "i = 0\n",
    "MAX_INDEX = 0\n",
    "for i,n in enumerate(reg_vocab):\n",
    "    \n",
    "    if len(n) > MAX_LENGTH:\n",
    "        MAX_LENGTH = len(n)\n",
    "        MAX_INDEX = i\n",
    "        \n",
    "print(MAX_LENGTH,MAX_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_to_char_seq(word):\n",
    "    word_chars = list(word)\n",
    "    word_chars_indices = np.array(list(map(lambda char: char_indices[char], word_chars)))\n",
    "    #return word_chars_indices\n",
    "    return sequence.pad_sequences([word_chars_indices], maxlen=MAX_LENGTH, padding='post')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_words(words, verbose=False):\n",
    "    vwords =[]\n",
    "    for name in words:\n",
    "        x = [char_indices[c] for c in name]\n",
    "        if verbose:\n",
    "            print(x)\n",
    "        vx = np.eye(len(char_indices))[x]  #https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.eye.html\n",
    "        if verbose:\n",
    "            print(vx[0:12])\n",
    "        vwords.append(vx)\n",
    "    return vwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stephen\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "y = np.array([model.wv[word[1:-1]] for word in reg_vocab])\n",
    "#del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X = np.array([word_to_char_seq(word) for word in reg_vocab], dtype='uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<particulate_opacity_mercury>',\n",
       " '<laundry_soap>',\n",
       " '<perming>',\n",
       " '<packaging_heat_shrinkable>',\n",
       " '<concerend>',\n",
       " '<pepper_spray_rubber_bullets>',\n",
       " '<----------------------------------------------------------_liabilities>',\n",
       " '<nonintervention>',\n",
       " '<rotations_lubrication>',\n",
       " '<velveteen_jacket>',\n",
       " '<grape_hyacinth>',\n",
       " '<defective_painkillers>',\n",
       " '<reattach_tendon>',\n",
       " '<juicier>',\n",
       " '<phospho>',\n",
       " '<worry_wart>',\n",
       " '<downloaden>',\n",
       " '<roosting>',\n",
       " '<ultrabright>',\n",
       " '<ingest>',\n",
       " '<retroviral_vector>',\n",
       " '<uncorrupt>',\n",
       " '<morphine_tablets>',\n",
       " '<unwound>',\n",
       " '<microwaving>',\n",
       " '<volgens>',\n",
       " '<dwarf_conifers>',\n",
       " '<fresh_garbanzos>',\n",
       " '<dilly_dallies>',\n",
       " '<antiwar_sentiments>',\n",
       " '<multitrack_recording>',\n",
       " '<craniofacial_anomalies>',\n",
       " '<starlets>',\n",
       " '<halibut_fishing>',\n",
       " '<refinery_coker>',\n",
       " '<vividly_recollect>',\n",
       " '<bustier_dress>',\n",
       " '<promo_pic>',\n",
       " '<lightning_bolt>',\n",
       " '<riskless_profit>',\n",
       " '<mistaking>',\n",
       " '<creamy_puree>',\n",
       " '<sensorium>',\n",
       " '<ragworm>',\n",
       " '<gender_disparity>',\n",
       " '<seemingly_hopeless>',\n",
       " '<translates>',\n",
       " '<cooters>',\n",
       " '<output_voltages>',\n",
       " '<based_cytogenetic_diagnostics>',\n",
       " '<rubber_gaskets>',\n",
       " '<bow_wows>',\n",
       " '<naman>',\n",
       " '<dilational>',\n",
       " '<elk_hunts>',\n",
       " '<gas_guzzling_jalopies>',\n",
       " '<ng_magandang>',\n",
       " '<psychosocial_counseling>',\n",
       " '<boundlessness>',\n",
       " '<wunnerful>',\n",
       " '<fats_vitamins_minerals>',\n",
       " '<superintelligent>',\n",
       " '<booby_trapped_orchards>',\n",
       " '<encaustic_paintings>',\n",
       " '<mutually_reinforcing_cycles>',\n",
       " '<timbered>',\n",
       " '<corrugated_iron_sheeting>',\n",
       " '<palindromes>',\n",
       " '<southbound_onramp>',\n",
       " '<leopardess>',\n",
       " '<colored_sweatshirt>',\n",
       " '<discus_sions>',\n",
       " '<lobster_claw_shoes>',\n",
       " '<foams>',\n",
       " '<pill_splitters>',\n",
       " '<amygdalin>',\n",
       " '<accross>',\n",
       " '<frontfoot>',\n",
       " '<stripper_pole>',\n",
       " '<lung_resections>',\n",
       " '<goodish_ground>',\n",
       " '<electrocatalysts>',\n",
       " '<latched>',\n",
       " '<thermal_tiles_exposing>',\n",
       " '<thermal_platesetter>',\n",
       " '<turmeric_coriander>',\n",
       " '<schoolboyish>',\n",
       " '<cov_ered>',\n",
       " '<biometric_credentialing>',\n",
       " '<candida_albicans>',\n",
       " '<mafic_dyke>',\n",
       " '<lib_dem_soc_commies>',\n",
       " '<categorical_denial>',\n",
       " '<processor>',\n",
       " '<metabolizing_enzymes>',\n",
       " '<aerosol_cans>',\n",
       " '<breast_enhancement>',\n",
       " '<meditative>',\n",
       " '<mermaid_hemline>',\n",
       " '<quieted_raucous>',\n",
       " '<placentals>',\n",
       " '<millefiori>',\n",
       " '<quartz_sericite>',\n",
       " '<capacitive_screen>',\n",
       " '<edging_nearer>',\n",
       " '<smudgy>',\n",
       " '<mcfpd>',\n",
       " '<faceless_bureaucrats>',\n",
       " '<oil_absorbant_boom>',\n",
       " '<sloop_sailing>',\n",
       " '<sisig>',\n",
       " '<planetary_alignments>',\n",
       " '<quelques>',\n",
       " '<remunerative_employment>',\n",
       " '<polluter_pays_principle>',\n",
       " '<talagang>',\n",
       " '<mummify>',\n",
       " '<single_dose_vials>',\n",
       " '<thje>',\n",
       " '<roulette_tables>',\n",
       " '<attendees>',\n",
       " '<orbicularis_oculi>',\n",
       " '<boneless_turkey>',\n",
       " '<uncoded>',\n",
       " '<arched_footbridge>',\n",
       " '<basins>',\n",
       " '<rocket_boosters>',\n",
       " '<draft>',\n",
       " '<peaches_melons>',\n",
       " '<mp3_wav>',\n",
       " '<gastroenterology_obstetrics_gynecology>',\n",
       " '<icicles_dangling>',\n",
       " '<monochromatic_color_palette>',\n",
       " '<inpatient_discharges>',\n",
       " '<water_dilutes_saltier>',\n",
       " '<crispy_onion>',\n",
       " '<iron_cookware>',\n",
       " '<panicked_concertgoers>',\n",
       " '<replacment>',\n",
       " '<lis_pendens>',\n",
       " '<econometric_forecasting>',\n",
       " '<rickety_contraption>',\n",
       " '<molding>',\n",
       " '<meboard>',\n",
       " '<sedex>',\n",
       " '<sprucing>',\n",
       " '<gracious_congratulatory>',\n",
       " '<ornate_ceilings>',\n",
       " '<commonplace_streptococcal_infection>',\n",
       " '<informe>',\n",
       " '<documentations>',\n",
       " '<str_=>',\n",
       " '<forbids_nonresidents>',\n",
       " \"<t_hey're>\",\n",
       " '<dipyridamole>',\n",
       " '<blogoshere>',\n",
       " '<piques>',\n",
       " '<abuts_raging_rapids>',\n",
       " '<dfs>',\n",
       " '<whiled>',\n",
       " '<truckmakers>',\n",
       " '<prerecorded_messages>',\n",
       " '<faa>',\n",
       " '<proprietary_intravenous_formulation>',\n",
       " '<reticle>',\n",
       " '<tastiest_morsels>',\n",
       " '<unwinds>',\n",
       " '<green_sparkle_booties>',\n",
       " '<threatened_desert_tortoise>',\n",
       " '<white_miters>',\n",
       " '<minnows_jigs>',\n",
       " '<zithromax>',\n",
       " '<unveils>',\n",
       " '<kidney_pancreas_transplants>',\n",
       " '<champagne_sipping>',\n",
       " '<blinkin>',\n",
       " '<hopple>',\n",
       " '<cadastre>',\n",
       " '<prepackaged_bankruptcy_proceeding>',\n",
       " '<drunkenness>',\n",
       " '<uncoated_freesheet_papers>',\n",
       " '<scandalise>',\n",
       " '<rehabilitate_foreclosed_homes>',\n",
       " '<scaup>',\n",
       " '<baile_funk>',\n",
       " '<solar_corona>',\n",
       " '<whats_ur>',\n",
       " '<boyhood_hero>',\n",
       " '<---------------_+>',\n",
       " '<circumstellar_disk>',\n",
       " '<riot_grrrl>',\n",
       " '<titanium_shavings>',\n",
       " '<buil_ding>',\n",
       " '<outer_rind>',\n",
       " '<governmental_approvals>',\n",
       " '<spring_chinook_salmon>',\n",
       " '<monosodium>',\n",
       " '<dural>',\n",
       " '<banditry_sabotage>',\n",
       " '<balsamic_roasted>',\n",
       " '<uncared>',\n",
       " '<changeable_faceplates>',\n",
       " '<beaver_lodge>',\n",
       " '<bare_breasted_maidens>',\n",
       " '<newsrelease>',\n",
       " '<spore_germination>',\n",
       " '<upregulated>',\n",
       " '<pettiness>',\n",
       " '<trammel>',\n",
       " '<rumen_bacteria>',\n",
       " '<sorely_underfunded>',\n",
       " '<migrating_northward>',\n",
       " '<cookies_cakes>',\n",
       " '<jus>',\n",
       " '<geological_modeling>',\n",
       " '<reinsurance_cessions>',\n",
       " '<leprosy_eradication>',\n",
       " '<loaded_superjumbo_jet>',\n",
       " '<enamel_hardness>',\n",
       " '<virtualized_datacenters>',\n",
       " '<megafauna>',\n",
       " '<sneaky>',\n",
       " '<wiser>',\n",
       " '<categorises>',\n",
       " '<ebonized>',\n",
       " '<leylandii_hedge>',\n",
       " '<bounderies>',\n",
       " '<churned_snow_plows>',\n",
       " '<bindery>',\n",
       " '<putted_atrociously>',\n",
       " '<santeros>',\n",
       " '<suppliers_hereby_disclaim>',\n",
       " '<sevoflurane>',\n",
       " '<hugest_fan>',\n",
       " '<subsidization>',\n",
       " '<ovenight>',\n",
       " '<pastern>',\n",
       " '<misprinted_ballots>',\n",
       " '<hydrogen_bioreactor>',\n",
       " '<snuffle>',\n",
       " '<defaces_majestic_scenery>',\n",
       " '<tomatoes_artichoke_hearts>',\n",
       " '<reasonable_inference>',\n",
       " '<weather_prognosticating_groundhog>',\n",
       " '<puritanism>',\n",
       " '<facial_twitch>',\n",
       " '<jubilating>',\n",
       " '<hereafter_referred>',\n",
       " '<inwardness>',\n",
       " '<secretive_polygamous_communities>',\n",
       " '<sportsmanlike_conduct>',\n",
       " '<litigated>',\n",
       " '<biomonitor_around>',\n",
       " '<electroless_nickel_immersion>',\n",
       " '<posseses>',\n",
       " '<tyring>',\n",
       " '<ulna_collateral_ligament>',\n",
       " '<fecal_material>',\n",
       " '<plywood_subfloor>',\n",
       " '<bro_mance>',\n",
       " '<leftist_guerrilla>',\n",
       " '<frontcourt_tandems>',\n",
       " '<jennifer_im_almost>',\n",
       " '<unloaded_handgun>',\n",
       " '<vegetative_buffers>',\n",
       " '<bopd>',\n",
       " '<cytidine_nucleoside_analog>',\n",
       " '<goriest>',\n",
       " '<oval_shaped>',\n",
       " '<gigantic_ape>',\n",
       " '<unadulterated_greed>',\n",
       " '<shrub_plantings>',\n",
       " '<categorically>',\n",
       " '<rudely_interrupt>',\n",
       " '<deterministic_nonlinear_measure>',\n",
       " '<signs_~_thepoint>',\n",
       " '<stakeout>',\n",
       " '<vinyl_fencing>',\n",
       " '<handout_photo_dated>',\n",
       " '<letters_testamentary>',\n",
       " '<socio_economic>',\n",
       " '<safest_therapeutically>',\n",
       " '<accessioning>',\n",
       " '<modernly>',\n",
       " '<investements>',\n",
       " '<organic_mulches>',\n",
       " '<hairy_chest>',\n",
       " '<palay_seeds>',\n",
       " '<tetrasodium>',\n",
       " '<transportable>',\n",
       " '<brew>',\n",
       " '<pharmacogenetic>',\n",
       " '<hiking_backpacking>',\n",
       " '<patellar_dislocation>',\n",
       " '<watt_incandescent_bulbs>',\n",
       " '<janjaweed_militias>',\n",
       " '<floggers>',\n",
       " '<sultry_soulful_rendition>',\n",
       " '<eyelid>',\n",
       " '<reelectionists>',\n",
       " '<opthalmology>',\n",
       " '<pharmeceutical>',\n",
       " '<sidewalls>',\n",
       " '<customer>',\n",
       " '<shopping_cart>',\n",
       " '<lyon>',\n",
       " '<hurling_stones>',\n",
       " '<grilled_chicken_sandwiches>',\n",
       " '<triangular_shapes>',\n",
       " '<green_chile_cheeseburgers>',\n",
       " '<strokeless>',\n",
       " '<rurals>',\n",
       " '<uneaten>',\n",
       " '<cashmere_sweaters_dresses>',\n",
       " '<bwah>',\n",
       " '<2bps_wider>',\n",
       " '<vapor_intrusion>',\n",
       " '<anomalous_uranium>',\n",
       " '<avid_bicyclist>',\n",
       " '<foot_telescoping_antenna>',\n",
       " '<poss>',\n",
       " '<deft_ballhandling>',\n",
       " '<epidurals>',\n",
       " '<delays>',\n",
       " '<incongruous_sight>',\n",
       " '<wrongfully_incarcerated>',\n",
       " '<ingredient_hydroxycitric_acid>',\n",
       " '<johnson>',\n",
       " '<sore_throats_itchy>',\n",
       " '<hamentashen>',\n",
       " '<abhors_namedropping>',\n",
       " '<pebbly_skin>',\n",
       " '<sicknote>',\n",
       " '<spousal_notification>',\n",
       " '<unpredictable_multicandidate_race>',\n",
       " '<signature_blond_pompadour>',\n",
       " '<blondeness>',\n",
       " '<thrilling_lyric_tenor>',\n",
       " '<jerry_built>',\n",
       " '<traps>',\n",
       " '<derailleurs>',\n",
       " '<evidentiary_objections>',\n",
       " '<graudate>',\n",
       " '<videotaped_hazing>',\n",
       " '<nitrates>',\n",
       " '<neoclassical_facade>',\n",
       " '<recuperated_enterprises>',\n",
       " '<shoes_fingerless_gloves>',\n",
       " '<replating>',\n",
       " '<animal_skins>',\n",
       " '<slashdotters>',\n",
       " '<rejected_absentee_ballots>',\n",
       " '<ammonia_refrigeration>',\n",
       " '<beady_eyes>',\n",
       " '<heaviest_fallers>',\n",
       " '<polar_orbit>',\n",
       " '<whooping_crane_migration>',\n",
       " '<nang_hindi>',\n",
       " '<boardwalk_arcade>',\n",
       " '<uninfluenced>',\n",
       " '<preceived>',\n",
       " '<dimpled_chads>',\n",
       " '<axon_degeneration>',\n",
       " '<conops>',\n",
       " '<per_tonne_gpt>',\n",
       " '<solitudes>',\n",
       " '<asparagus_fern>',\n",
       " '<carred>',\n",
       " '<spaceflight>',\n",
       " '<tanda>',\n",
       " '<cellulite>',\n",
       " '<cheers_resounded>',\n",
       " '<unseen_footage>',\n",
       " '<ill>',\n",
       " '<fan_avidity>',\n",
       " '<sedition>',\n",
       " '<firewall_antivirus>',\n",
       " '<ungroup>',\n",
       " '<lushness>',\n",
       " '<skiing_sledding>',\n",
       " '<pineapple_daisies>',\n",
       " '<irritability_mood_swings>',\n",
       " '<academic_rigor>',\n",
       " '<lensed_cameras>',\n",
       " '<performers>',\n",
       " '<spaghetti_macaroni>',\n",
       " '<normalizing_monetary>',\n",
       " '<rightfield_wall>',\n",
       " '<large_quanities>',\n",
       " '<ergosterol>',\n",
       " '<expansionary_stance>',\n",
       " '<wind_rustling>',\n",
       " '<overdramatized>',\n",
       " '<amorous_encounters>',\n",
       " '<groovier>',\n",
       " '<pharaonic_tombs_discovered>',\n",
       " '<marajuana>',\n",
       " '<eyes_misting>',\n",
       " '<court_enforceable_undertakings>',\n",
       " '<mas_marami>',\n",
       " '<fruity_cocktail>',\n",
       " '<pointillistic>',\n",
       " '<coronary_revascularization_procedures>',\n",
       " '<linden_blossom>',\n",
       " '<samarium_cobalt>',\n",
       " '<unbeaten>',\n",
       " '<bispecific_antibodies>',\n",
       " '<optical_lithography_techniques>',\n",
       " '<sewage_backups>',\n",
       " '<red_tracer_bullets>',\n",
       " '<delicately_choreographed_effort>',\n",
       " '<electronic_subsystems_instrumentation>',\n",
       " '<fierce_competitor>',\n",
       " '<microwell>',\n",
       " '<masectomy>',\n",
       " '<leylandii>',\n",
       " '<chai_lattes>',\n",
       " '<ataxic>',\n",
       " '<spineless_coward>',\n",
       " '<chem>',\n",
       " '<wearing_bib_overalls>',\n",
       " '<uplink_ports>',\n",
       " '<oems>',\n",
       " '<absorbable_sutures>',\n",
       " '<extracted_groundnut>',\n",
       " '<misstatement>',\n",
       " '<nearthe>',\n",
       " '<hypericin>',\n",
       " '<juuuust>',\n",
       " '<ethnic_purges>',\n",
       " '<coincident_indicators>',\n",
       " '<lactating_cow>',\n",
       " '<wholetime_directors>',\n",
       " '<tthat>',\n",
       " '<en_bloc>',\n",
       " '<betrayers>',\n",
       " '<skim_milk_powder>',\n",
       " '<lung_carcinomas>',\n",
       " '<hanh>',\n",
       " '<robotic_geologists>',\n",
       " '<planned_suitors_shoplifter>',\n",
       " '<covalent>',\n",
       " '<wh_*>',\n",
       " '<bacon_sandwich>',\n",
       " '<maile_lei>',\n",
       " '<cybersleuths>',\n",
       " '<tired_gantry_cranes>',\n",
       " '<agents>',\n",
       " '<inky_purple>',\n",
       " '<horned_cattle>',\n",
       " '<crags>',\n",
       " '<licking_lollipop>',\n",
       " '<epoetin_delta>',\n",
       " '<nike_dunk>',\n",
       " '<cf_d>',\n",
       " '<unwanted_attentions>',\n",
       " '<hob_nob>',\n",
       " '<liquidambar_trees>',\n",
       " '<cyhoeddus>',\n",
       " '<wear_insect_repellent>',\n",
       " '<muscle_invasive_papillary>',\n",
       " '<arenite>',\n",
       " '<skewing>',\n",
       " '<freshies>',\n",
       " '<unceremoniously_turfed>',\n",
       " '<bleakness>',\n",
       " '<tightrope_walkers>',\n",
       " '<cytostatics>',\n",
       " '<deadine>',\n",
       " '<papyrus_reeds>',\n",
       " '<religious_obscurantism>',\n",
       " '<blotter>',\n",
       " '<seersucker_pants>',\n",
       " '<avocado_growers>',\n",
       " '<artery_elasticity>',\n",
       " '<milega>',\n",
       " '<dropdowns>',\n",
       " '<leaking_radioactive_tritium>',\n",
       " '<butanol>',\n",
       " '<crossmember>',\n",
       " '<millimeterwave_applications>',\n",
       " '<hemodialysis_patients>',\n",
       " '<iniatives>',\n",
       " '<dallied>',\n",
       " '<enterpriser_staff>',\n",
       " '<alcoholic_stupor>',\n",
       " '<eyelash_glue>',\n",
       " '<nicotinic_alpha_7>',\n",
       " '<ferns_mosses>',\n",
       " '<synth_hooks>',\n",
       " '<topbonus>',\n",
       " '<oxy_coal>',\n",
       " '<manning_kerosene_lamps>',\n",
       " '<coincidentally>',\n",
       " '<lapsation>',\n",
       " '<martial_arts_practitioner>',\n",
       " '<suspend_repeat_cussers>',\n",
       " '<plantar_fascia>',\n",
       " '<marrow_transplantation>',\n",
       " '<excessively_pessimistic>',\n",
       " '<offense>',\n",
       " '<teflon_coated>',\n",
       " '<fired_rubber_bullets>',\n",
       " '<jaunts>',\n",
       " '<undertow>',\n",
       " '<activitists>',\n",
       " '<knot_breeze>',\n",
       " '<glovemen>',\n",
       " '<godmother>',\n",
       " '<pried_loose>',\n",
       " '<grooves>',\n",
       " '<khayals>',\n",
       " '<semiconductor_photomasks>',\n",
       " '<icey>',\n",
       " '<chronic_periodontitis>',\n",
       " '<putaways>',\n",
       " '<peak_lapel_tuxedo>',\n",
       " '<hateful_hurtful>',\n",
       " '<mock_turtleneck>',\n",
       " '<viagra_generic>',\n",
       " '<piccolo_flute>',\n",
       " '<include_idling_styrene>',\n",
       " '<chart_datum>',\n",
       " '<brian>',\n",
       " '<windswept_shores>',\n",
       " '<goth_chicks>',\n",
       " '<intransigent_attitude>',\n",
       " '<commutable_distance>',\n",
       " '<distilled_vodka>',\n",
       " '<smiles_shyly>',\n",
       " '<unforgotten>',\n",
       " '<likeliest>',\n",
       " '<topic_vulgar_profane>',\n",
       " '<otaku>',\n",
       " '<obscurer>',\n",
       " '<tweeners>',\n",
       " '<instant_messaging_conferencing>',\n",
       " '<bln_nkr_vs>',\n",
       " '<wheat_sowings>',\n",
       " '<mangrove_tunnels>',\n",
       " '<lazily_drifting>',\n",
       " '<opposing_ballhandlers>',\n",
       " '<lows>',\n",
       " '<sidelong>',\n",
       " '<overhasty>',\n",
       " '<hormone_leptin>',\n",
       " '<morally_blameworthy>',\n",
       " '<nako>',\n",
       " '<maternal_uncles>',\n",
       " '<undervalued>',\n",
       " '<collared_dove>',\n",
       " '<doublewide_mobile>',\n",
       " '<gawping>',\n",
       " '<acute_decompensation>',\n",
       " '<pea_sized>',\n",
       " '<grossly_deformed>',\n",
       " '<folksy>',\n",
       " '<bunad>',\n",
       " '<profanity_laced_finger_pointing>',\n",
       " '<synthetic_pyrethroid>',\n",
       " '<unparalled>',\n",
       " '<jazz_sax>',\n",
       " '<wealthy_benefactors>',\n",
       " '<baby_wholphin>',\n",
       " '<balancesheet>',\n",
       " '<coordinately>',\n",
       " '<submicron_sized>',\n",
       " '<initiate_roving_wiretaps>',\n",
       " '<thermophiles>',\n",
       " '<guilting>',\n",
       " '<white_checkered_tablecloth>',\n",
       " '<antlers>',\n",
       " '<chaperones_rearranged>',\n",
       " \"<qu'en>\",\n",
       " '<theproposed>',\n",
       " '<raging_lunatic>',\n",
       " '<subtleness>',\n",
       " '<metalizing>',\n",
       " '<sticky_gunk_coating>',\n",
       " '<whets_appetite>',\n",
       " '<horseshoe_shaped>',\n",
       " '<central_precocious_puberty>',\n",
       " '<called_serial_overdrafters>',\n",
       " '<par_4s>',\n",
       " '<breaching_conditional_discharge>',\n",
       " '<onder_meer>',\n",
       " '<hen_mallard>',\n",
       " '<decommission>',\n",
       " '<declares_mistrial>',\n",
       " '<microvolt>',\n",
       " '<chopped_kale>',\n",
       " '<riding_unicycle>',\n",
       " '<excitation>',\n",
       " '<pagri>',\n",
       " '<feigned_tickle>',\n",
       " '<notoriously_picky>',\n",
       " '<plot>',\n",
       " '<booed_heartily>',\n",
       " '<unemployed_tree_trimmer>',\n",
       " '<globular_star>',\n",
       " '<megawatt_pulverized_coal>',\n",
       " '<cardio_respiratory_assistance>',\n",
       " '<inflated_nightcrawlers>',\n",
       " '<phonebook>',\n",
       " '<rattan_furniture>',\n",
       " '<tempo_reale>',\n",
       " '<dermatitis_eczema>',\n",
       " '<comedy>',\n",
       " '<vacuum_gripper>',\n",
       " '<cannoli>',\n",
       " '<alcopops>',\n",
       " '<ryanodine_receptor>',\n",
       " '<disempowered>',\n",
       " '<cutups>',\n",
       " '<chows>',\n",
       " '<includes_deluxe_accommodations>',\n",
       " '<rotundifolia>',\n",
       " '<vocal_tic>',\n",
       " '<methylmercury_poisoning>',\n",
       " '<superfamily>',\n",
       " '<steward>',\n",
       " '<frowny_face>',\n",
       " '<tachograph_charts>',\n",
       " '<internationally_acclaimed>',\n",
       " '<aus>',\n",
       " '<yield_curve>',\n",
       " '<magnanimous>',\n",
       " '<isbeing>',\n",
       " '<pre_synaptic_inhibition>',\n",
       " '<crossbones>',\n",
       " '<taluka_panchayats>',\n",
       " '<tattoos_birthmarks>',\n",
       " '<bulls_**>',\n",
       " '<hydrochloric_acid_spilled>',\n",
       " '<pipevine_swallowtail>',\n",
       " '<cette>',\n",
       " '<sheepshead_flounder>',\n",
       " '<soft_calendered>',\n",
       " '<thatching>',\n",
       " '<convincing>',\n",
       " '<severest_heatwave>',\n",
       " '<merchantman>',\n",
       " '<mocha_vanilla>',\n",
       " '<nuclear_arsenal>',\n",
       " '<surgeons_anesthesiologists_nurses>',\n",
       " '<marshmallow_bunnies>',\n",
       " '<paid_glowing_tributes>',\n",
       " '<mediakit>',\n",
       " '<orifice>',\n",
       " '<mechanizations>',\n",
       " '<selenium_yeast>',\n",
       " \"<young'n>\",\n",
       " '<toasted_almond_slivers>',\n",
       " '<immunocompromised_patients>',\n",
       " '<spiders>',\n",
       " '<pert_bum>',\n",
       " '<silage_pits>',\n",
       " '<tripping>',\n",
       " '<grover>',\n",
       " '<look_familiarly_fearsome>',\n",
       " '<globally_interconnected_colossus>',\n",
       " '<0_&&>',\n",
       " '<anthrax_mailer>',\n",
       " '<printer_cartridge_recycling>',\n",
       " '<diamond_encrusted_skulls>',\n",
       " '<tling>',\n",
       " '<unguided_rockets>',\n",
       " '<cataloged_consigned>',\n",
       " '<interdict>',\n",
       " '<flatwoods>',\n",
       " '<pythagorean>',\n",
       " '<embryo_clones>',\n",
       " '<homerless_drought>',\n",
       " '<sodium_bromide>',\n",
       " '<sinong>',\n",
       " '<fishtail_skirt>',\n",
       " '<5mg>',\n",
       " '<presure>',\n",
       " '<cookery_demonstration>',\n",
       " '<analysts_surveyed>',\n",
       " '<doesn_`_t>',\n",
       " '<serenade>',\n",
       " '<disperse_stone_pelting>',\n",
       " '<highlight_reel_worthy>',\n",
       " '<loitering_ordinance>',\n",
       " '<je_te>',\n",
       " '<fester_unchecked>',\n",
       " '<gaily_lit>',\n",
       " '<ints>',\n",
       " '<coral_triangle>',\n",
       " '<nacho_cheese>',\n",
       " '<hatch>',\n",
       " '<nematode_worm>',\n",
       " '<white_hooped>',\n",
       " '<animal_behaviorist>',\n",
       " '<civic_boosters>',\n",
       " '<gunmetal>',\n",
       " '<magnetic_susceptibility>',\n",
       " '<blondie>',\n",
       " '<scalped_inauguration>',\n",
       " '<replaceable_batteries>',\n",
       " '<equal_aplomb>',\n",
       " '<hardbound>',\n",
       " '<toxic_pesticides>',\n",
       " '<pliers_screwdrivers>',\n",
       " '<irrational_hatred>',\n",
       " '<steep_dropoff>',\n",
       " '<cardioembolic>',\n",
       " '<groundnut_seed>',\n",
       " '<sheer_immensity>',\n",
       " '<interworkings>',\n",
       " '<vaporetto>',\n",
       " '<guestrooms>',\n",
       " '<included_miniskirt_thong>',\n",
       " '<fruity_pebbles>',\n",
       " '<potato_chips>',\n",
       " '<underpaid_underappreciated>',\n",
       " '<mishap_occurred>',\n",
       " '<dissemblers>',\n",
       " '<parasitic_twin>',\n",
       " '<picnic_benches>',\n",
       " '<fertilizers_pesticides_herbicides>',\n",
       " '<collagen_fibril>',\n",
       " '<whodunnits>',\n",
       " '<glass_paned>',\n",
       " '<comforters_blankets>',\n",
       " '<curiosity_seekers>',\n",
       " '<cadmium_selenium>',\n",
       " '<t3>',\n",
       " '<ventilate_manure_pits>',\n",
       " '<bouncier_wickets>',\n",
       " '<earlie>',\n",
       " '<&tstr;>',\n",
       " '<galvanometer>',\n",
       " '<discontinues>',\n",
       " '<floorpan>',\n",
       " '<unashamedly_sentimental>',\n",
       " '<unscripted_spontaneous>',\n",
       " '<outstretched_leg>',\n",
       " '<zeke>',\n",
       " '<regionally_sourced>',\n",
       " '<sheer_ubiquity>',\n",
       " '<aortic_dissection>',\n",
       " '<destabilized>',\n",
       " '<wase>',\n",
       " '<immune_suppression>',\n",
       " '<storm_wallops>',\n",
       " '<wonton_noodles>',\n",
       " '<mask_aligner>',\n",
       " '<parleying>',\n",
       " '<ghatam>',\n",
       " '<infector>',\n",
       " '<i7>',\n",
       " '<receptiveness>',\n",
       " '<recasts>',\n",
       " '<hydrogen_internal_combustion>',\n",
       " '<pattie>',\n",
       " '<grubs_worms>',\n",
       " '<polyethylene>',\n",
       " '<pater_familias>',\n",
       " '<flexographic_printing>',\n",
       " '<and3>',\n",
       " '<malaria_endemic>',\n",
       " '<alerce>',\n",
       " '<virtually_unpoliced>',\n",
       " '<making_terroristic_threats>',\n",
       " '<tables_brows_furrowed>',\n",
       " '<postural_instability>',\n",
       " '<moonlight_stroll>',\n",
       " '<animal_dissections>',\n",
       " '<wearing_baggy_jeans>',\n",
       " '<investigational_drug>',\n",
       " '<asian_tits>',\n",
       " '<cheap_soma>',\n",
       " '<soaraway>',\n",
       " '<airstrikes_artillery_shelling>',\n",
       " '<adrenoceptor_agonist>',\n",
       " '<landfills_incinerators>',\n",
       " '<antedates>',\n",
       " '<nimble_adversary>',\n",
       " '<coagulation_disorders>',\n",
       " '<dissenting_vote>',\n",
       " '<detenues>',\n",
       " '<elite_sportspeople>',\n",
       " '<conferece>',\n",
       " '<tow_trucks>',\n",
       " '<westerly_wind>',\n",
       " '<oft_parodied>',\n",
       " '<faecal>',\n",
       " '<sweatshirts_hats>',\n",
       " '<internalising>',\n",
       " '<bitter_taste>',\n",
       " '<seigniorage>',\n",
       " '<doji_candle>',\n",
       " '<snyder>',\n",
       " '<aligned>',\n",
       " '<voodoo_lily>',\n",
       " '<gondii>',\n",
       " '<pin_striping>',\n",
       " '<decommission_sectarian>',\n",
       " '<lymphatic_endothelial_cells>',\n",
       " '<succes>',\n",
       " '<nonsolicitation>',\n",
       " '<clubhead>',\n",
       " '<waging_war>',\n",
       " '<repetitive_behaviors_compulsions>',\n",
       " '<acute_respiratory_infections>',\n",
       " '<fydd_yn>',\n",
       " '<seismologically_active>',\n",
       " '<thermonuclear_reactions>',\n",
       " '<optique>',\n",
       " '<onbase>',\n",
       " '<mold_mildew_bacteria>',\n",
       " '<topless_bathing>',\n",
       " '<diphtheria_pertussis>',\n",
       " '<thwart_hostile_takeovers>',\n",
       " '<phosphorescent_glow>',\n",
       " '<mmo>',\n",
       " '<contain_phonetic_spellings>',\n",
       " '<cabernet_sauvignon>',\n",
       " '<tablespoons_melted_butter>',\n",
       " '<trimmed>',\n",
       " '<mga_opisyal>',\n",
       " '<euonymus>',\n",
       " '<prefilter>',\n",
       " '<coke_binge>',\n",
       " '<ashcan>',\n",
       " '<painful_sores>',\n",
       " '<sleek_ergonomic>',\n",
       " '<protrude>',\n",
       " '<interproximal>',\n",
       " '<beaded_gowns>',\n",
       " '<colonic_hydrotherapy>',\n",
       " '<gupshup>',\n",
       " '<fluidized_catalytic_cracking>',\n",
       " '<folksy_bluesy>',\n",
       " '<hickory_nuts>',\n",
       " '<goles>',\n",
       " '<intertemporal_budget>',\n",
       " '<lilting_ballad>',\n",
       " '<dump_truck>',\n",
       " '<gentle_caress>',\n",
       " '<plug_ins>',\n",
       " '<iced_tea_lemonade>',\n",
       " '<circuiting>',\n",
       " '<inducible_ischemia>',\n",
       " '<slithering>',\n",
       " '<mozzarella_cheeses>',\n",
       " '<fluid_catalytic_cracker>',\n",
       " '<hydra_headed_beast>',\n",
       " '<sonic_boom_blew>',\n",
       " '<contractually_obliged>',\n",
       " '<rentable_square_footage>',\n",
       " '<impenetrable_jargon>',\n",
       " '<textural_contrast>',\n",
       " '<tankfuls>',\n",
       " '<celebratory_margaritas>',\n",
       " '<give_befitting_reply>',\n",
       " '<pulsed_plasma>',\n",
       " '<peti>',\n",
       " '<standy>',\n",
       " '<hundreds_ofthousands>',\n",
       " '<brivaracetam>',\n",
       " '<bedliner>',\n",
       " '<nearness>',\n",
       " '<taggants>',\n",
       " '<seek_redress>',\n",
       " '<culinarian>',\n",
       " '<inecalcitol>',\n",
       " '<extravagance_epitomized>',\n",
       " '<genomic_profiling>',\n",
       " '<innovative_fungicides_insecticides>',\n",
       " '<serological_tests>',\n",
       " '<coporations>',\n",
       " '<seeming_contradictions>',\n",
       " '<opposite>',\n",
       " '<deadly_vermiculite_mine>',\n",
       " '<trademark_beehive_hairstyle>',\n",
       " '<ancien>',\n",
       " '<cosmetically_altered>',\n",
       " '<khakis>',\n",
       " '<disfigurement>',\n",
       " '<shem>',\n",
       " '<et_cetera_et_cetera>',\n",
       " '<downright_dishonest>',\n",
       " '<icefall>',\n",
       " '<cocreator>',\n",
       " '<logging_keystrokes>',\n",
       " '<unmanned_airships>',\n",
       " '<chaise_lounge_lay>',\n",
       " '<poison_dart_frogs>',\n",
       " '<psychopathic_murderers>',\n",
       " '<surreal_interlude>',\n",
       " '<pero_hindi>',\n",
       " '<mortar_bombs>',\n",
       " '<tarts>',\n",
       " '<analyst_mentions>',\n",
       " '<methamphetamines>',\n",
       " '<linage>',\n",
       " '<falsely_advertised>',\n",
       " '<mesial_temporal_lobe_epilepsy>',\n",
       " '<bearded_monk>',\n",
       " '<archery_muzzleloader>',\n",
       " '<ration_cardholders>',\n",
       " '<pectoralis_muscle>',\n",
       " '<pit_bulls>',\n",
       " '<awhile>',\n",
       " '<sentinel_node_biopsy>',\n",
       " '<rhinestone_trimmed>',\n",
       " '<life_assurers>',\n",
       " '<bada_platform>',\n",
       " '<painting_printmaking_sculpture>',\n",
       " '<telemarketing_identity_theft>',\n",
       " '<thalidomide>',\n",
       " '<harpists>',\n",
       " '<peanuts_almonds_pecans>',\n",
       " '<crippleware>',\n",
       " '<quicken>',\n",
       " '<lethal_injection_inhumane>',\n",
       " '<keema>',\n",
       " '<pepper_cayenne_pepper>',\n",
       " '<millionaire_thrill_seeker>',\n",
       " '<politi>',\n",
       " '<pulverized>',\n",
       " '<anomalous_dispersion>',\n",
       " '<de_bureaucratisation>',\n",
       " '<disagree>',\n",
       " '<bbi>',\n",
       " '<multiphoton>',\n",
       " '<lateral_meniscus_cartilage>',\n",
       " '<facililty>',\n",
       " '<kayakers_rafters>',\n",
       " '<turbot>',\n",
       " '<veiny>',\n",
       " '<legible_handwriting>',\n",
       " '<silica_alunite_alteration>',\n",
       " '<buy_viagra>',\n",
       " '<astronomically_expensive>',\n",
       " '<unpowered_glide>',\n",
       " '<eco_friendly>',\n",
       " '<flood_alleviation>',\n",
       " '<seizing_weapons_caches>',\n",
       " '<falsifying_timecards>',\n",
       " '<lemon_tart>',\n",
       " '<dye_sublimation_printer>',\n",
       " '<eastbound_semitrailer>',\n",
       " '<sunt>',\n",
       " '<ignorant_slob>',\n",
       " '<nanometer_wavelength>',\n",
       " '<bright_sunshine>',\n",
       " '<felucca>',\n",
       " '<civil_liberties_campaigners>',\n",
       " '<-1_-2>',\n",
       " '<filthy_rags>',\n",
       " '<belts_hoses>',\n",
       " '<rearranging_furniture>',\n",
       " '<popularizing_gas_guzzling>',\n",
       " '<assesses>',\n",
       " '<resynchronization>',\n",
       " '<erect_billboard>',\n",
       " '<resin_coated>',\n",
       " '<caramelizes>',\n",
       " '<bourne>',\n",
       " '<egg_citement>',\n",
       " '<metabolically_engineered>',\n",
       " '<aminotransferase_levels>',\n",
       " '<supersport>',\n",
       " '<cabernet>',\n",
       " '<polystyrene_containers>',\n",
       " '<imputation>',\n",
       " '<poppadoms>',\n",
       " '<keyhole_limpet>',\n",
       " '<horse_breeder>',\n",
       " '<crow_flies>',\n",
       " '<ciabatta_bread>',\n",
       " '<filed_writ_petition>',\n",
       " '<reprivatization>',\n",
       " '<9sec>',\n",
       " '<hake_fishery>',\n",
       " '<realizes>',\n",
       " '<subsea_wellhead>',\n",
       " '<puking>',\n",
       " '<reveal_intricately_braided>',\n",
       " '<beltway_bandits>',\n",
       " '<sulphide_bearing>',\n",
       " '<bruised_egos>',\n",
       " '<airfreight_forwarder>',\n",
       " '<protectionists>',\n",
       " '<localness>',\n",
       " '<diamond_studded_jewelery>',\n",
       " '<grassy_meadows>',\n",
       " '<elective_despotism>',\n",
       " '<tres_bon>',\n",
       " '<pulses_oilseeds>',\n",
       " '<taluk_panchayat>',\n",
       " '<waxed_eloquent>',\n",
       " '<diethylene_glycol_thickening>',\n",
       " '<neti_pot>',\n",
       " '<psoriatic_plaques>',\n",
       " '<suicide_vests>',\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reg_vocab, y)\n",
    "#del X\n",
    "del y\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array([word_to_char_seq(word) for word in X_train], dtype='uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(323078, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cos_plus_absolute(ytrue, ypred):\n",
    "    return losses.mean_absolute_error(ytrue, ypred) + losses.cosine_proximity(ytrue, ypred)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "k_model = Sequential()\n",
    "onehot_weights = np.eye(num_classes)\n",
    "k_model.add(Embedding(num_classes, num_classes,\n",
    "                      trainable=False,\n",
    "                      input_length=98,\n",
    "                      name='onehot'))\n",
    "# k_model.add(Conv1D(filters=16, kernel_size=8, strides=1, padding='valid'))\n",
    "# k_model.add(MaxPooling1D(pool_size=2))\n",
    "# k_model.add(Conv1D(filters=16, kernel_size=5, strides=1, padding='valid'))\n",
    "# k_model.add(MaxPooling1D(pool_size=2))\n",
    "# k_model.add(Conv1D(filters=16, kernel_size=3, strides=1, padding='valid'))\n",
    "# k_model.add(MaxPooling1D(pool_size=2))\n",
    "k_model.add(Flatten())\n",
    "#k_model.add(Embedding(1,300))\n",
    "\n",
    "k_model.add(Dense(300, activation='sigmoid'))\n",
    "k_model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "                \n",
    "          \n",
    "k_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "onehot (Embedding)           (None, 100, 100)          10000     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 99, 16)            3216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 49, 16)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 300)               235500    \n",
      "=================================================================\n",
      "Total params: 248,716\n",
      "Trainable params: 238,716\n",
      "Non-trainable params: 10,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "num_classes = len(char_list)\n",
    "k_model = Sequential()\n",
    "#k_model.add(OneHot(len(printable)))\n",
    "#k_model.add(Input(shape=(98,), dtype='uint8'))\n",
    "#k_model.add(Lambda(K.one_hot, arguments={'num_classes': 100}, input_shape=(98,), output_shape=(98,100)))\n",
    "# k_model.add(Lambda(to_categorical(100)))(input_shape=(1, 98))\n",
    "onehot_weights = np.eye(num_classes)\n",
    "k_model.add(Embedding(num_classes, num_classes,\n",
    "                      trainable=False,\n",
    "                      embeddings_initializer=keras.initializers.Identity(gain=1.0),\n",
    "                      input_length=MAX_LENGTH,\n",
    "                      #weights=[onehot_weights],\n",
    "                      name='onehot'))\n",
    "k_model.add(Conv1D(filters=16, kernel_size=2, strides=1, padding='valid'))\n",
    "k_model.add(MaxPooling1D(pool_size=2))\n",
    "k_model.add(Flatten())\n",
    "\n",
    "k_model.add(Dense(300, activation='relu'))\n",
    "k_model.compile(loss=cos_plus_absolute,\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy', 'mean_squared_error', 'cosine_proximity', 'mean_absolute_error'])\n",
    "\n",
    "k_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 100)          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "onehot (Embedding)              (None, 100, 100)     10000       masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 100, 8)       1608        onehot[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 100, 32)      9632        onehot[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 100, 16)      6416        onehot[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 100, 16)      8016        onehot[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 100, 16)      9616        onehot[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 100, 88)      0           conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 50, 88)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 48, 64)       16960       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 24, 64)       0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1536)         0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1000)         1537000     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 500)          500500      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 400)          200400      dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "out (Dense)                     (None, 300)          120300      dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,420,448\n",
      "Trainable params: 2,410,448\n",
      "Non-trainable params: 10,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "num_classes = len(printable)\n",
    "#k_model.add(OneHot(len(printable)))\n",
    "#k_model.add(Input(shape=(98,), dtype='uint8'))\n",
    "#k_model.add(Lambda(K.one_hot, arguments={'num_classes': 100}, input_shape=(98,), output_shape=(98,100)))\n",
    "# k_model.add(Lambda(to_categorical(100)))(input_shape=(1, 98))\n",
    "onehot_weights = np.eye(num_classes)\n",
    "\n",
    "# k_model.add(Conv1D(filters=16, kernel_size=10 strides=1, padding='valid'))\n",
    "# k_model.add(MaxPooling1D(pool_size=2))\n",
    "i = Input(shape=(MAX_LENGTH,))\n",
    "mask = Masking(0)(i)\n",
    "one_hot = Embedding(num_classes, num_classes,\n",
    "                      trainable=False,\n",
    "                      embeddings_initializer=keras.initializers.Identity(gain=1.0),\n",
    "                      #weights=[onehot_weights],\n",
    "                      name='onehot')(mask)\n",
    "small = Conv1D(filters=8, kernel_size=2, strides=1, padding='same')(one_hot)\n",
    "med = Conv1D(filters=32, kernel_size=3, strides=1, padding='same')(one_hot)\n",
    "large = Conv1D(filters=16, kernel_size=4, strides=1, padding='same')(one_hot)\n",
    "vlarge = Conv1D(filters=16, kernel_size=5, strides=1, padding='same')(one_hot)\n",
    "huge = Conv1D(filters=16, kernel_size=6, strides=1, padding='same')(one_hot)\n",
    "#hugest = Conv1D(filters=16, kernel_size=7, strides=1, padding='same')(one_hot)\n",
    "\n",
    "# small_pool = MaxPooling1D(pool_size=2)(small)\n",
    "# med_pool = MaxPooling1D(pool_size=2)(med)\n",
    "# large_pool = MaxPooling1D(pool_size=2)(large)\n",
    "\n",
    "# small_flatten = Flatten()(small_pool)\n",
    "# med_flatten = Flatten()(med_pool)\n",
    "# large_flatten = Flatten()(large_pool)\n",
    "\n",
    "merge = concatenate([small, med, large, vlarge, huge], axis=-1)\n",
    "\n",
    "pool = MaxPooling1D(pool_size=2)(merge)\n",
    "\n",
    "mergeConv = Conv1D(filters=64, kernel_size=3, strides=1)(pool)\n",
    "mergePool = MaxPooling1D(pool_size=2)(mergeConv)\n",
    "full_flatten = Flatten()(mergePool)\n",
    "dense1 = Dense(1000, activation='relu')(full_flatten)\n",
    "dense2 = Dense(500, activation='relu')(dense1)\n",
    "dense3 = Dense(400, activation='relu')(dense2)\n",
    "\n",
    "\n",
    "out = Dense(300, activation='tanh', name='out')(dense3)\n",
    "\n",
    "k_model = Model(inputs=i, outputs=out)\n",
    "\n",
    "k_model.compile(loss=cos_plus_absolute,\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy', 'mean_squared_error', 'cosine_proximity', 'mean_absolute_error'])\n",
    "\n",
    "k_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 290770 samples, validate on 32308 samples\n",
      "Epoch 1/50\n",
      "290770/290770 [==============================] - 180s 621us/step - loss: -0.2679 - acc: 0.0345 - mean_squared_error: 0.0208 - cosine_proximity: -0.3707 - mean_absolute_error: 0.1029 - val_loss: -0.2905 - val_acc: 0.0379 - val_mean_squared_error: 0.0200 - val_cosine_proximity: -0.3912 - val_mean_absolute_error: 0.1006\n",
      "Epoch 2/50\n",
      "290770/290770 [==============================] - 176s 606us/step - loss: -0.2998 - acc: 0.0432 - mean_squared_error: 0.0202 - cosine_proximity: -0.4008 - mean_absolute_error: 0.1010 - val_loss: -0.3063 - val_acc: 0.0461 - val_mean_squared_error: 0.0198 - val_cosine_proximity: -0.4063 - val_mean_absolute_error: 0.1000\n",
      "Epoch 3/50\n",
      "290770/290770 [==============================] - 176s 606us/step - loss: -0.3147 - acc: 0.0481 - mean_squared_error: 0.0200 - cosine_proximity: -0.4151 - mean_absolute_error: 0.1004 - val_loss: -0.3155 - val_acc: 0.0496 - val_mean_squared_error: 0.0197 - val_cosine_proximity: -0.4151 - val_mean_absolute_error: 0.0996\n",
      "Epoch 4/50\n",
      "290770/290770 [==============================] - 177s 608us/step - loss: -0.3278 - acc: 0.0532 - mean_squared_error: 0.0198 - cosine_proximity: -0.4277 - mean_absolute_error: 0.0999 - val_loss: -0.3214 - val_acc: 0.0524 - val_mean_squared_error: 0.0196 - val_cosine_proximity: -0.4207 - val_mean_absolute_error: 0.0994\n",
      "Epoch 5/50\n",
      "290770/290770 [==============================] - 177s 609us/step - loss: -0.3400 - acc: 0.0579 - mean_squared_error: 0.0196 - cosine_proximity: -0.4394 - mean_absolute_error: 0.0994 - val_loss: -0.3260 - val_acc: 0.0534 - val_mean_squared_error: 0.0195 - val_cosine_proximity: -0.4252 - val_mean_absolute_error: 0.0992\n",
      "Epoch 6/50\n",
      "290770/290770 [==============================] - 184s 633us/step - loss: -0.3517 - acc: 0.0627 - mean_squared_error: 0.0195 - cosine_proximity: -0.4506 - mean_absolute_error: 0.0989 - val_loss: -0.3299 - val_acc: 0.0550 - val_mean_squared_error: 0.0194 - val_cosine_proximity: -0.4289 - val_mean_absolute_error: 0.0991\n",
      "Epoch 7/50\n",
      "290770/290770 [==============================] - 183s 630us/step - loss: -0.3625 - acc: 0.0659 - mean_squared_error: 0.0193 - cosine_proximity: -0.4609 - mean_absolute_error: 0.0984 - val_loss: -0.3315 - val_acc: 0.0564 - val_mean_squared_error: 0.0194 - val_cosine_proximity: -0.4306 - val_mean_absolute_error: 0.0991\n",
      "Epoch 8/50\n",
      "290770/290770 [==============================] - 189s 651us/step - loss: -0.3720 - acc: 0.0693 - mean_squared_error: 0.0192 - cosine_proximity: -0.4701 - mean_absolute_error: 0.0980 - val_loss: -0.3335 - val_acc: 0.0569 - val_mean_squared_error: 0.0193 - val_cosine_proximity: -0.4325 - val_mean_absolute_error: 0.0990\n",
      "Epoch 9/50\n",
      "290770/290770 [==============================] - 195s 669us/step - loss: -0.3810 - acc: 0.0727 - mean_squared_error: 0.0190 - cosine_proximity: -0.4787 - mean_absolute_error: 0.0976 - val_loss: -0.3339 - val_acc: 0.0565 - val_mean_squared_error: 0.0193 - val_cosine_proximity: -0.4329 - val_mean_absolute_error: 0.0991\n",
      "Epoch 10/50\n",
      "290770/290770 [==============================] - 195s 670us/step - loss: -0.3890 - acc: 0.0752 - mean_squared_error: 0.0189 - cosine_proximity: -0.4862 - mean_absolute_error: 0.0973 - val_loss: -0.3333 - val_acc: 0.0575 - val_mean_squared_error: 0.0193 - val_cosine_proximity: -0.4324 - val_mean_absolute_error: 0.0991\n",
      "Epoch 11/50\n",
      "290770/290770 [==============================] - 195s 670us/step - loss: -0.3961 - acc: 0.0778 - mean_squared_error: 0.0188 - cosine_proximity: -0.4931 - mean_absolute_error: 0.0970 - val_loss: -0.3330 - val_acc: 0.0583 - val_mean_squared_error: 0.0193 - val_cosine_proximity: -0.4322 - val_mean_absolute_error: 0.0992\n",
      "Epoch 12/50\n",
      "290770/290770 [==============================] - 195s 670us/step - loss: -0.4027 - acc: 0.0801 - mean_squared_error: 0.0187 - cosine_proximity: -0.4994 - mean_absolute_error: 0.0967 - val_loss: -0.3331 - val_acc: 0.0590 - val_mean_squared_error: 0.0194 - val_cosine_proximity: -0.4324 - val_mean_absolute_error: 0.0993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1636cab38>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(strftime(\"%a-%H-%M-%S\")))\n",
    "\n",
    "k_model.fit(X_train, y_train,\n",
    "                    batch_size=1000,\n",
    "                    epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "#                    callbacks=[tensorboard])\n",
    "                    callbacks=[tensorboard,\n",
    "                               EarlyStopping(monitor='val_loss',\n",
    "                                              min_delta=.0001,\n",
    "                                              patience=3,\n",
    "                                              verbose=0, mode='auto')])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "k_model.save('1CNN-f64-k3.h5')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from random import sample\n",
    "sample_size = 50\n",
    "# poserrors = []\n",
    "# negerrors = []\n",
    "for i, word in enumerate(sample(X_test, sample_size)):\n",
    "    #print(word)\n",
    "#     ms = model.most_similar(k_model.predict(np.array([word_to_char_seq(word)])), topn=10000000000)\n",
    "#     print('ms done')\n",
    "#     #intermediatreg_vocab_sete = list(filter(lambda item: item[0] in reg_vocab, ms)) #[item[0] for item in ms if item[0] in reg_vocab]\n",
    "#     print('inter done')\n",
    "    poserrors += [[item[0] for item in model.most_similar(positive=k_model.predict(np.array([word_to_char_seq(word)])), topn=10000000) if item[0] in reg_vocab_set].index(word)]\n",
    "    negerrors += [[item[0] for item in model.most_similar(negative=k_model.predict(np.array([word_to_char_seq(word)])), topn=10000000) if item[0] in reg_vocab_set].index(word)]\n",
    "\n",
    "    print(poserrors[-1]/len(reg_vocab_set), sum(poserrors)/len(poserrors)/len(reg_vocab_set), word)\n",
    "print(sum(poserrors)/sample_size/len(reg_vocab_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21h 32min 20s\n"
     ]
    }
   ],
   "source": [
    "from random import sample\n",
    "\n",
    "# k_model._make_predict_function()\n",
    "# graph = K.get_session().graph\n",
    "\n",
    "# def func(word): \n",
    "#     print(word)\n",
    "#     return [item[0] for item in model.most_similar(positive=k_model.predict(np.array([word_to_char_seq(word)])), topn=10000000) if item[0] in reg_vocab_set].index(word)\n",
    "def makepred(word):\n",
    "    return k_model.predict(np.array([word_to_char_seq(word[1:-1])]))\n",
    "def getindex(pred):\n",
    "    vec = pred[0]\n",
    "    word = pred[1][1:-1]\n",
    "    return [item[0] for item in model.most_similar(vec, topn=3000000) if '<' + item[0] + '>' in reg_vocab_set].index(word)\n",
    "\n",
    "preds = [(makepred(word), word) for word in sample(X_test, 15000)]\n",
    "#indices = [getindex(vec) for vec in preds]\n",
    "\n",
    "p = Pool(16)\n",
    "\n",
    "%time out = list(map(getindex, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3213411453726767\n",
      "0.280798505402873\n",
      "0.24494104756355467\n"
     ]
    }
   ],
   "source": [
    "from statistics import *\n",
    "outperc = [i/len(reg_vocab_set) for i in out]\n",
    "print(mean(outperc))\n",
    "print(stdev(outperc))\n",
    "\n",
    "print(median(outperc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "430771"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reg_vocab_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ot"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Anaconda3]",
   "language": "python",
   "name": "Python [Anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
